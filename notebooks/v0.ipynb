{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install av\n!pip install rarfile\n!pip install wget","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRaLmieZU80f","outputId":"945ee5cb-fa5b-440a-c6b4-c0740e1d8286","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T05:32:58.960469Z","iopub.execute_input":"2025-02-07T05:32:58.960702Z","iopub.status.idle":"2025-02-07T05:33:08.686583Z","shell.execute_reply.started":"2025-02-07T05:32:58.960681Z","shell.execute_reply":"2025-02-07T05:33:08.685543Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.1.0)\nRequirement already satisfied: rarfile in /usr/local/lib/python3.10/dist-packages (4.2)\nRequirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport ssl\nimport wget\nimport urllib.request\nimport rarfile\nimport zipfile\n\ncontext = ssl._create_unverified_context()\ndef download_url(url, path):\n    print(f\"downloading {url}...\")\n    with urllib.request.urlopen(url, context=context) as response:\n        with open(path, 'wb') as f:\n            f.write(response.read())\n\ndef main():\n    data_dir = \"data\"\n    os.makedirs(data_dir, exist_ok=True)\n\n    video_url = \"https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\"\n    annotation_url = \"https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip\"\n\n    download_url(video_url, os.path.join(data_dir, \"UCF101.rar\"))\n    download_url(annotation_url, os.path.join(data_dir, \"UCF101_annotations.zip\"))\n\n    print(\"extracting video data...\")\n    rf = rarfile.RarFile(os.path.join(data_dir, \"UCF101.rar\"))\n    rf.extractall(data_dir)\n\n    print(\"extracting annotations...\")\n    with zipfile.ZipFile(os.path.join(data_dir, \"UCF101_annotations.zip\")) as zf:\n        zf.extractall(data_dir)\n\n    os.remove(os.path.join(data_dir, \"UCF101.rar\"))\n    os.remove(os.path.join(data_dir, \"UCF101_annotations.zip\"))\n\n    print(\"downloading ast...\")\n    os.makedirs('pretrained_weights', exist_ok=True)\n    wget.download('https://www.dropbox.com/s/mdsa4t1xmcimia6/audioset_16_16_0.4422.pth?dl=1', os.path.join('pretrained_weights', 'audioset_16_16_0.4422.pth'))\n    print(\"done\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZ73EYkzTbHb","outputId":"697c04ba-bb9f-4c2d-c29b-315990822498","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:14:23.512598Z","iopub.execute_input":"2025-02-06T18:14:23.512961Z","iopub.status.idle":"2025-02-06T18:19:32.638415Z","shell.execute_reply.started":"2025-02-06T18:14:23.512922Z","shell.execute_reply":"2025-02-06T18:19:32.637413Z"}},"outputs":[{"name":"stdout","text":"downloading https://www.crcv.ucf.edu/data/UCF101/UCF101.rar...\ndownloading https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip...\nextracting video data...\nextracting annotations...\ndownloading ast...\ndone\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torchaudio\nfrom pathlib import Path\nimport torchvision.io as io\n\n\nclass UCF101Dataset(Dataset):\n    def __init__(self, data_path, split_path, split=\"train\", num_frames=8, t=4):\n        self.data_path = Path(data_path)\n        self.num_frames = num_frames\n        self.t = t\n        # read split file\n        split_file = \"trainlist01.txt\" if split == \"train\" else \"testlist01.txt\"\n        with open(os.path.join(split_path, split_file), \"r\") as f:\n            self.video_list = [line.strip().split(\" \")[0] for line in f.readlines()]\n\n        # TODO add more randomizations\n        self.video_transform = transforms.Compose(\n            [\n                transforms.Resize((256, 256)), # slightly larger for random crop\n                transforms.RandomCrop(224),\n                transforms.RandomHorizontalFlip(),\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n\n        # create class to index mapping\n        self.class_to_idx = {}\n        classes = sorted(\n            list(set(video_name.split(\"/\")[0] for video_name in self.video_list))\n        )\n        for idx, classname in enumerate(classes):\n            self.class_to_idx[classname] = idx\n\n    def _load_video(self, video_path):\n        try:\n            vframes, _, _ = io.read_video(str(video_path), pts_unit=\"sec\")\n            total_frames = len(vframes)\n\n            # ensure we don't sample beyond video length\n            if total_frames < self.num_frames:\n                indices = torch.linspace(0, total_frames - 1, total_frames).long()\n                indices = torch.cat(\n                    [\n                        indices,\n                        torch.tensor(\n                            [total_frames - 1] * (self.num_frames - total_frames)\n                        ),\n                    ]\n                )\n            else:\n                indices = torch.linspace(0, total_frames - 1, self.num_frames).long()\n\n            frames = []\n            for idx in indices:\n                frame = vframes[idx]\n                frame = Image.fromarray(frame.numpy())\n                frame = self.video_transform(frame)\n                frames.append(frame)\n\n        except Exception as e:\n            print(e)\n            frames = [torch.zeros(3, 224, 224) for _ in range(self.num_frames)]\n\n        return torch.stack(frames)  # [num_frames, c, h, w]\n\n    # TODO specaugment\n    def _load_audio(self, video_path):\n        try:\n            audio_array, sample_rate = torchaudio.load(str(video_path))\n        except (RuntimeError, TypeError):\n            # create a small amount of noise instead of pure zeros\n            audio_array = torch.randn(1, 16000 * self.t) * 1e-4\n            sample_rate = 16000\n\n        # convert to mono\n        if audio_array.shape[0] > 1:\n            audio_array = torch.mean(audio_array, dim=0, keepdim=True)\n\n        # resample if necessary\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n            audio_array = resampler(audio_array)\n\n        target_length = self.t * 16000\n        if audio_array.shape[1] < target_length:\n            # pad with zeros if audio is too short\n            audio_array = torch.nn.functional.pad(\n                audio_array, (0, target_length - audio_array.shape[1])\n            )\n        else:\n            # trim if audio is too long\n            audio_array = audio_array[:, :target_length]\n\n        # create mel spectrogram\n        spectrogram = torchaudio.transforms.MelSpectrogram(\n            sample_rate=16000,\n            n_mels=128,\n            n_fft=1024,\n            win_length=1024,\n            hop_length=160,\n        )(audio_array)\n\n        spectrogram = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n        spectrogram = spectrogram.squeeze(0)  # remove channel dimension\n\n        if spectrogram.shape[1] > 400:\n            spectrogram = spectrogram[:, :400]\n        elif spectrogram.shape[1] < 400:\n            spectrogram = torch.nn.functional.pad(\n                spectrogram, (0, 400 - spectrogram.shape[1])\n            )\n\n        # mean=0 std=0.5 according to ast\n        spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6) * 0.5\n\n        return spectrogram.unsqueeze(0)  # add channel dimension back [1, 128, 100*t]\n\n    def __getitem__(self, idx):\n        video_name = self.video_list[idx]\n        video_path = self.data_path / \"UCF-101\" / video_name\n\n        label = video_name.split(\"/\")[0]\n        video_tensor = self._load_video(video_path)\n        audio_tensor = self._load_audio(video_path)\n        class_idx = self.class_to_idx[label]\n\n        return video_tensor, audio_tensor, class_idx\n\n    def __len__(self):\n        return len(self.video_list)\n\n\ndef get_dataloaders(data_path, split_path, batch_size=2, num_workers=0):\n\n    train_dataset = UCF101Dataset(\n        data_path=data_path,\n        split_path=os.path.join(split_path, \"ucfTrainTestlist\"),\n        split=\"train\",\n    )\n\n    val_dataset = UCF101Dataset(\n        data_path=data_path,\n        split_path=os.path.join(split_path, \"ucfTrainTestlist\"),\n        split=\"test\",\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_loader, val_loader\n","metadata":{"id":"X3fAn8D2Tb0I","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T05:33:09.933075Z","iopub.execute_input":"2025-02-07T05:33:09.933550Z","iopub.status.idle":"2025-02-07T05:33:16.700283Z","shell.execute_reply.started":"2025-02-07T05:33:09.933511Z","shell.execute_reply":"2025-02-07T05:33:16.699324Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\nfrom timm.layers import trunc_normal_\n\n\n# TODO change hardcoded values\nclass PatchEmbed(nn.Module):\n    def __init__(\n        self, img_size=(128, 400), patch_size=(16, 16), in_chans=1, embed_dim=768\n    ):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert (\n            H == self.img_size[0] and W == self.img_size[1]\n        ), f\"input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})\"\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass Model(nn.Module):\n    def __init__(self, num_classes=101, lf=10):\n        super().__init__()\n\n        # set num_classes=0 to remove classification head\n        self.vv = timm.create_model(\n            \"vit_base_patch16_224.augreg_in21k\", pretrained=True, num_classes=0\n        )\n\n        self.va = timm.create_model(\n            \"vit_base_patch16_224.augreg_in21k\", pretrained=True, num_classes=0\n        )\n\n        # apply ast weights to va\n        ast_weights = torch.load(\"pretrained_weights/audioset_16_16_0.4422.pth\", weights_only=True)\n        temp = self.va.state_dict()\n        pretrained_dict = {}\n        for k, v in ast_weights.items():\n            if k.startswith(\"module.\"):\n                k = k[7:]\n            if k in temp and temp[k].shape == v.shape:\n                pretrained_dict[k] = v\n        temp.update(pretrained_dict)\n        self.va.load_state_dict(temp)\n\n        self.va.patch_embed = PatchEmbed(\n            img_size=(128, 400),\n            patch_size=(16, 16),\n            in_chans=1,\n            embed_dim=self.va.embed_dim,\n        )\n\n        # interpolate position embeddings for video\n        num_patches_video = self.vv.patch_embed.num_patches\n        self.vv.pos_embed = self.interpolate_pos_encoding(\n            self.vv.pos_embed, num_patches_video\n        )\n\n        # interpolate position embeddings for audio\n        num_patches_audio = self.va.patch_embed.num_patches\n        self.va.pos_embed = self.interpolate_pos_encoding(\n            self.va.pos_embed, num_patches_audio\n        )\n\n        # create new positional embeddings for fused sequence\n        total_patches = num_patches_video + num_patches_audio + 2\n        self.fused_pos_embed = nn.Parameter(\n            torch.zeros(1, total_patches, self.vv.embed_dim)\n        )\n        trunc_normal_(self.fused_pos_embed, std=0.02)\n\n        self.lf = lf\n        self.num_features = self.vv.embed_dim\n\n        # create new classification head for fused features\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(self.num_features),\n            nn.Linear(self.num_features, num_classes),\n        )\n\n    def interpolate_pos_encoding(self, pos_embed, num_patches):\n        pos_embed = pos_embed.float()\n        N = pos_embed.shape[1] - 1  # original number of patches (excluding CLS token)\n\n        # handle CLS token separately\n        cls_pos_embed = pos_embed[:, 0:1, :]\n        pos_embed = pos_embed[:, 1:, :]\n\n        # interpolate patch position embeddings\n        pos_embed = pos_embed.permute(0, 2, 1)\n        pos_embed = nn.functional.interpolate(\n            pos_embed, size=num_patches, mode=\"linear\", align_corners=False\n        )\n        pos_embed = pos_embed.permute(0, 2, 1)\n\n        # recombine with CLS token\n        pos_embed = torch.cat((cls_pos_embed, pos_embed), dim=1)\n        return nn.Parameter(pos_embed)\n\n    def forward_features(self, x, v, lf):\n        B = x.shape[0]\n        x = v.patch_embed(x)\n        if len(x.shape) > 3:  # just in case\n            x = x.flatten(2).transpose(1, 2)\n        cls_token = v.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_token, x), dim=1)\n        x = v.pos_drop(x + v.pos_embed)\n\n        for i, block in enumerate(v.blocks):\n            if i < lf:\n                x = block(x)\n        return x\n\n    def forward(self, video, audio):\n        B, F, c, h, w = video.shape\n        video = video.view(B * F, c, h, w)\n\n        # process separately until fusion layer\n        v_features = self.forward_features(video, self.vv, self.lf)\n        v_features = v_features.view(B, F, -1, self.num_features)\n        v_features = torch.mean(v_features, dim=1)\n        a_features = self.forward_features(audio, self.va, self.lf)\n\n        fused = torch.cat((v_features, a_features), dim=1)\n\n        # add fused positional embeddings after concatenation\n        fused = fused + self.fused_pos_embed\n\n        # pass through remaining layers\n        for i in range(self.lf, len(self.vv.blocks)):\n            fused = self.vv.blocks[i](fused)\n\n        v_cls = fused[:, 0]\n        a_cls = fused[:, self.vv.patch_embed.num_patches + 1]\n\n        v_logits = self.classifier(v_cls)\n        a_logits = self.classifier(a_cls)\n\n        output = (v_logits + a_logits) / 2\n\n        return output\n","metadata":{"id":"udRiJHAlTdLe","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T05:33:16.701419Z","iopub.execute_input":"2025-02-07T05:33:16.701847Z","iopub.status.idle":"2025-02-07T05:33:19.842584Z","shell.execute_reply.started":"2025-02-07T05:33:16.701821Z","shell.execute_reply":"2025-02-07T05:33:19.841868Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport datetime\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport wandb\n\n\nclass Trainer:\n    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n        wandb.login(key=\"key\")\n\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.device = device\n\n        if wandb.run is None:\n            current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            run_name = f\"wabt_{current_time}\"\n\n            wandb.init(project=\"wabt\", name=run_name)\n            wandb.watch(self.model)\n        self.step = 0\n\n        self.best_val_loss = float(\"inf\")\n        self.checkpoint_dir = os.path.join(\"checkpoints\", run_name)\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n    def train_epoch(self):\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        pbar = tqdm(self.train_loader, desc=\"training\")\n        for video, audio, targets in pbar:\n            video = video.to(self.device)\n            audio = audio.to(self.device)\n            targets = targets.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.model(video, audio)\n            loss = self.criterion(outputs, targets)\n\n            loss.backward()\n            self.optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            self.step += 1\n            wandb.log(\n                {\n                    \"batch/train_loss\": loss.item(),\n                    \"batch/train_acc\": predicted.eq(targets).sum().item()\n                    / targets.size(0),\n                    \"train/running_loss\": running_loss / (pbar.n + 1),\n                    \"train/running_acc\": correct / total,\n                    \"step\": self.step,\n                }\n            )\n\n            pbar.set_postfix(\n                {\"loss\": running_loss / (pbar.n + 1), \"acc\": correct / total}\n            )\n\n        epoch_loss = running_loss / len(self.train_loader)\n        epoch_acc = correct / total\n        return epoch_loss, epoch_acc\n\n    def validate(self):\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            pbar = tqdm(self.val_loader, desc=\"validation\")\n            for video, audio, targets in pbar:\n                video = video.to(self.device)\n                audio = audio.to(self.device)\n                targets = targets.to(self.device)\n\n                outputs = self.model(video, audio)\n                loss = self.criterion(outputs, targets)\n\n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n                self.step += 1\n                wandb.log(\n                    {\n                        \"batch/val_loss\": loss.item(),\n                        \"batch/val_acc\": predicted.eq(targets).sum().item()\n                        / targets.size(0),\n                        \"val/running_loss\": running_loss / (pbar.n + 1),\n                        \"step\": self.step,\n                    }\n                )\n\n                pbar.set_postfix(\n                    {\"loss\": running_loss / (pbar.n + 1), \"acc\": correct / total}\n                )\n\n        epoch_loss = running_loss / len(self.val_loader)\n        epoch_acc = correct / total\n\n        # save checkpoint\n        checkpoint = {\n            \"model_state_dict\": self.model.state_dict(),\n            \"optimizer_state_dict\": self.optimizer.state_dict(),\n            \"val_loss\": epoch_loss,\n            \"val_acc\": epoch_acc,\n            \"step\": self.step,\n        }\n\n        # save latest checkpoint\n        latest_path = os.path.join(self.checkpoint_dir, \"latest.pt\")\n        torch.save(checkpoint, latest_path)\n\n        # save best checkpoint\n        if epoch_loss < self.best_val_loss:\n            self.best_val_loss = epoch_loss\n            best_path = os.path.join(self.checkpoint_dir, \"best.pt\")\n            torch.save(checkpoint, best_path)\n            wandb.log({\"best_val_loss\": self.best_val_loss})\n\n        return epoch_loss, epoch_acc\n\n\ndef train(model, train_loader, val_loader, num_epochs=10, device=\"cuda\"):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n    trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)\n\n    for epoch in range(num_epochs):\n        print(f\"\\nepoch {epoch+1}/{num_epochs}\")\n        train_loss, train_acc = trainer.train_epoch()\n        val_loss, val_acc = trainer.validate()\n\n        wandb.log(\n            {\n                \"epoch\": epoch + 1,\n                \"epoch/train_loss\": train_loss,\n                \"epoch/train_acc\": train_acc,\n                \"epoch/val_loss\": val_loss,\n                \"epoch/val_acc\": val_acc,\n            }\n        )\n\n        print(f\"train loss: {train_loss:.4f} | train acc: {train_acc:.2f}\")\n        print(f\"val loss: {val_loss:.4f} | val acc: {val_acc:.2f}\")\n","metadata":{"id":"SP90zlKMTgK8","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:22:04.915483Z","iopub.execute_input":"2025-02-06T18:22:04.915766Z","iopub.status.idle":"2025-02-06T18:22:04.931056Z","shell.execute_reply.started":"2025-02-06T18:22:04.915744Z","shell.execute_reply":"2025-02-06T18:22:04.930130Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = Model().to(device)\ntrain_loader, val_loader = get_dataloaders(data_path=\"data/\", split_path=\"data/\")\n\ntrain(model, train_loader, val_loader, num_epochs=5, device=device)","metadata":{"id":"y1K4x0qKThWZ","colab":{"base_uri":"https://localhost:8080/","height":434},"outputId":"4d656f62-9348-4e93-82f6-1ae95b0d8776","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T18:22:08.801700Z","iopub.execute_input":"2025-02-06T18:22:08.802054Z","iopub.status.idle":"2025-02-06T23:48:40.051562Z","shell.execute_reply.started":"2025-02-06T18:22:08.802022Z","shell.execute_reply":"2025-02-06T23:48:40.050577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/410M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"686abe3edcbd412baf814d28e39d58be"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msouhhmm\u001b[0m (\u001b[33msouhhmm-bits-pilani\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250206_182221-kk37e3kl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/souhhmm-bits-pilani/wabt/runs/kk37e3kl' target=\"_blank\">wabt_20250206_182221</a></strong> to <a href='https://wandb.ai/souhhmm-bits-pilani/wabt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/souhhmm-bits-pilani/wabt' target=\"_blank\">https://wandb.ai/souhhmm-bits-pilani/wabt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/souhhmm-bits-pilani/wabt/runs/kk37e3kl' target=\"_blank\">https://wandb.ai/souhhmm-bits-pilani/wabt/runs/kk37e3kl</a>"},"metadata":{}},{"name":"stdout","text":"\nepoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"training: 100%|██████████| 4769/4769 [51:30<00:00,  1.54it/s, loss=1.82, acc=0.543]  \nvalidation: 100%|██████████| 1892/1892 [13:04<00:00,  2.41it/s, loss=1.18, acc=0.681]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 1.8238 | train acc: 0.54\nval loss: 1.1784 | val acc: 0.68\n\nepoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"training: 100%|██████████| 4769/4769 [51:52<00:00,  1.53it/s, loss=0.351, acc=0.892] \nvalidation: 100%|██████████| 1892/1892 [13:12<00:00,  2.39it/s, loss=0.909, acc=0.76] \n","output_type":"stream"},{"name":"stdout","text":"train loss: 0.3510 | train acc: 0.89\nval loss: 0.9093 | val acc: 0.76\n\nepoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"training: 100%|██████████| 4769/4769 [51:50<00:00,  1.53it/s, loss=0.165, acc=0.95] \nvalidation: 100%|██████████| 1892/1892 [13:28<00:00,  2.34it/s, loss=1.11, acc=0.721]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 0.1650 | train acc: 0.95\nval loss: 1.1142 | val acc: 0.72\n\nepoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"training: 100%|██████████| 4769/4769 [51:53<00:00,  1.53it/s, loss=0.0549, acc=0.985]\nvalidation: 100%|██████████| 1892/1892 [13:08<00:00,  2.40it/s, loss=0.85, acc=0.792] \n","output_type":"stream"},{"name":"stdout","text":"train loss: 0.0549 | train acc: 0.98\nval loss: 0.8501 | val acc: 0.79\n\nepoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"training: 100%|██████████| 4769/4769 [52:35<00:00,  1.51it/s, loss=0.0332, acc=0.991]\nvalidation: 100%|██████████| 1892/1892 [13:14<00:00,  2.38it/s, loss=0.876, acc=0.799]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 0.0332 | train acc: 0.99\nval loss: 0.8756 | val acc: 0.80\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def load_model(checkpoint_path, device=\"cuda\"):\n    model = Model().to(device)\n    checkpoint = torch.load(checkpoint_path, weights_only=True)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    return model\n\n\ndef get_class_mapping():\n    # create a dataset instance just to get the class mapping\n    dataset = UCF101Dataset(data_path=\"data/\", split_path=\"data/ucfTrainTestlist\", split=\"test\")\n    # invert the class_to_idx dictionary\n    idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n    return idx_to_class\n\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # load model\n    checkpoint_path = \"checkpoints/wabt_20250206_182221/best.pt\"\n    model = load_model(checkpoint_path, device)\n\n    # get class mapping\n    idx_to_class = get_class_mapping()\n\n    # create dataset and get a sample\n    dataset = UCF101Dataset(\n        data_path=\"data/\",\n        split_path=\"data/ucfTrainTestlist\",\n        split=\"test\",\n    )\n\n    random_idxs = torch.randperm(len(dataset))[:10]\n    for i in random_idxs:\n        video, audio, true_label = dataset[i]\n\n        # add batch dimension\n        video = video.unsqueeze(0).to(device)\n        audio = audio.unsqueeze(0).to(device)\n\n        # get prediction\n        with torch.no_grad():\n            outputs = model(video, audio)\n            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n\n            # get top 3 predictions\n            top_prob, top_idx = torch.topk(probabilities, 3)\n\n        print(f\"\\nsample {i + 1}\")\n        print(f\"true class: {idx_to_class[true_label]}\")\n        print(\"top 3 predictions:\")\n        for prob, idx in zip(top_prob[0], top_idx[0]):\n            print(f\"{idx_to_class[idx.item()]}: {prob.item()*100:.2f}%\")\n        print(\"-\" * 50)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T05:48:35.337874Z","iopub.execute_input":"2025-02-07T05:48:35.338220Z","iopub.status.idle":"2025-02-07T05:48:40.736876Z","shell.execute_reply.started":"2025-02-07T05:48:35.338189Z","shell.execute_reply":"2025-02-07T05:48:40.736049Z"}},"outputs":[{"name":"stdout","text":"\nsample 1746\ntrue class: JugglingBalls\ntop 3 predictions:\nJugglingBalls: 75.22%\nNunchucks: 24.11%\nPlayingGuitar: 0.36%\n--------------------------------------------------\n\nsample 3686\ntrue class: WallPushups\ntop 3 predictions:\nWritingOnBoard: 90.42%\nYoYo: 1.41%\nHammering: 1.38%\n--------------------------------------------------\n\nsample 1988\ntrue class: MilitaryParade\ntop 3 predictions:\nMilitaryParade: 99.43%\nHorseRace: 0.50%\nVolleyballSpiking: 0.04%\n--------------------------------------------------\n\nsample 382\ntrue class: BenchPress\ntop 3 predictions:\nBenchPress: 99.06%\nPunch: 0.44%\nCleanAndJerk: 0.22%\n--------------------------------------------------\n\nsample 1193\ntrue class: FrontCrawl\ntop 3 predictions:\nFrontCrawl: 98.00%\nBreastStroke: 1.92%\nSkyDiving: 0.02%\n--------------------------------------------------\n\nsample 3380\ntrue class: TaiChi\ntop 3 predictions:\nBaseballPitch: 88.55%\nSalsaSpin: 4.56%\nTaiChi: 4.49%\n--------------------------------------------------\n\nsample 656\ntrue class: BoxingPunchingBag\ntop 3 predictions:\nBoxingPunchingBag: 98.87%\nCricketBowling: 0.43%\nShavingBeard: 0.21%\n--------------------------------------------------\n\nsample 870\ntrue class: CricketShot\ntop 3 predictions:\nCricketBowling: 93.99%\nCricketShot: 5.49%\nBaseballPitch: 0.17%\n--------------------------------------------------\n\nsample 1598\ntrue class: HorseRiding\ntop 3 predictions:\nHorseRiding: 99.61%\nHorseRace: 0.21%\nWalkingWithDog: 0.15%\n--------------------------------------------------\n\nsample 3381\ntrue class: TaiChi\ntop 3 predictions:\nBaseballPitch: 62.80%\nTaiChi: 13.84%\nSalsaSpin: 10.13%\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport torchaudio\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef analyze_dataset(data_path, split_path):\n    train_file = os.path.join(split_path, \"ucfTrainTestlist\", \"trainlist01.txt\")\n    test_file = os.path.join(split_path, \"ucfTrainTestlist\", \"testlist01.txt\")\n    \n    with open(train_file, \"r\") as f:\n        train_videos = [line.strip().split(\" \")[0] for line in f.readlines()]\n    \n    with open(test_file, \"r\") as f:\n        test_videos = [line.strip().split(\" \")[0] for line in f.readlines()]\n    \n    data_path = Path(data_path)\n    \n    def check_audio(video_list):\n        has_audio = 0\n        no_audio = 0\n        \n        for video in tqdm(video_list):\n            video_path = data_path / \"UCF-101\" / video\n            try:\n                audio_array, _ = torchaudio.load(str(video_path))\n                if audio_array.shape[1] > 0:  # check if audio has content\n                    has_audio += 1\n                else:\n                    no_audio += 1\n            except:\n                no_audio += 1\n        \n        return has_audio, no_audio\n    \n    print(\"analyzing training split...\")\n    train_has_audio, train_no_audio = check_audio(train_videos)\n    \n    print(\"analyzing test split...\")\n    test_has_audio, test_no_audio = check_audio(test_videos)\n    \n    # create visualization\n    labels = ['Training Set', 'Test Set']\n    has_audio = [train_has_audio, test_has_audio]\n    no_audio = [train_no_audio, test_no_audio]\n    \n    x = range(len(labels))\n    width = 0.35\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar([i - width/2 for i in x], has_audio, width, label='Has Audio')\n    ax.bar([i + width/2 for i in x], no_audio, width, label='No Audio')\n    \n    ax.set_ylabel('Number of Videos')\n    ax.set_title('Audio Availability in UCF101 Dataset')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels)\n    ax.legend()\n\n    for i in x:\n        ax.text(i - width/2, has_audio[i], str(has_audio[i]), ha='center', va='bottom')\n        ax.text(i + width/2, no_audio[i], str(no_audio[i]), ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig('audio_availability.png')\n    plt.close()\n    \n    print(\"\\nSummary:\")\n    print(f\"Training Set - Total: {len(train_videos)}\")\n    print(f\"  - With Audio: {train_has_audio} ({train_has_audio/len(train_videos)*100:.1f}%)\")\n    print(f\"  - Without Audio: {train_no_audio} ({train_no_audio/len(train_videos)*100:.1f}%)\")\n    \n    print(f\"\\nTest Set - Total: {len(test_videos)}\")\n    print(f\"  - With Audio: {test_has_audio} ({test_has_audio/len(test_videos)*100:.1f}%)\")\n    print(f\"  - Without Audio: {test_no_audio} ({test_no_audio/len(test_videos)*100:.1f}%)\")\n\nif __name__ == \"__main__\":\n    analyze_dataset(\"data/\", \"data/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T05:48:56.824437Z","iopub.execute_input":"2025-02-07T05:48:56.824780Z","iopub.status.idle":"2025-02-07T05:50:16.645690Z","shell.execute_reply.started":"2025-02-07T05:48:56.824751Z","shell.execute_reply":"2025-02-07T05:50:16.644902Z"}},"outputs":[{"name":"stdout","text":"analyzing training split...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9537/9537 [00:57<00:00, 167.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"analyzing test split...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3783/3783 [00:22<00:00, 167.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nSummary:\nTraining Set - Total: 9537\n  - With Audio: 4893 (51.3%)\n  - Without Audio: 4644 (48.7%)\n\nTest Set - Total: 3783\n  - With Audio: 1944 (51.4%)\n  - Without Audio: 1839 (48.6%)\n","output_type":"stream"}],"execution_count":9}]}
