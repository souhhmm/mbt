{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-02-08T11:22:13.043400Z","iopub.status.busy":"2025-02-08T11:22:13.043070Z","iopub.status.idle":"2025-02-08T11:22:22.855826Z","shell.execute_reply":"2025-02-08T11:22:22.854471Z","shell.execute_reply.started":"2025-02-08T11:22:13.043357Z"},"id":"WRaLmieZU80f","outputId":"945ee5cb-fa5b-440a-c6b4-c0740e1d8286","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.1.0)\n","Requirement already satisfied: rarfile in /usr/local/lib/python3.10/dist-packages (4.2)\n","Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"]}],"source":["!pip install av\n","!pip install rarfile\n","!pip install wget"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-02-06T18:14:23.512961Z","iopub.status.busy":"2025-02-06T18:14:23.512598Z","iopub.status.idle":"2025-02-06T18:19:32.638415Z","shell.execute_reply":"2025-02-06T18:19:32.637413Z","shell.execute_reply.started":"2025-02-06T18:14:23.512922Z"},"id":"XZ73EYkzTbHb","outputId":"697c04ba-bb9f-4c2d-c29b-315990822498","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading https://www.crcv.ucf.edu/data/UCF101/UCF101.rar...\n","downloading https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip...\n","extracting video data...\n","extracting annotations...\n","downloading ast...\n","done\n"]}],"source":["import os\n","import ssl\n","import wget\n","import urllib.request\n","import rarfile\n","import zipfile\n","\n","context = ssl._create_unverified_context()\n","def download_url(url, path):\n","    print(f\"downloading {url}...\")\n","    with urllib.request.urlopen(url, context=context) as response:\n","        with open(path, 'wb') as f:\n","            f.write(response.read())\n","\n","def main():\n","    data_dir = \"data\"\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    video_url = \"https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\"\n","    annotation_url = \"https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip\"\n","\n","    download_url(video_url, os.path.join(data_dir, \"UCF101.rar\"))\n","    download_url(annotation_url, os.path.join(data_dir, \"UCF101_annotations.zip\"))\n","\n","    print(\"extracting video data...\")\n","    rf = rarfile.RarFile(os.path.join(data_dir, \"UCF101.rar\"))\n","    rf.extractall(data_dir)\n","\n","    print(\"extracting annotations...\")\n","    with zipfile.ZipFile(os.path.join(data_dir, \"UCF101_annotations.zip\")) as zf:\n","        zf.extractall(data_dir)\n","\n","    os.remove(os.path.join(data_dir, \"UCF101.rar\"))\n","    os.remove(os.path.join(data_dir, \"UCF101_annotations.zip\"))\n","\n","    print(\"downloading ast...\")\n","    os.makedirs('pretrained_weights', exist_ok=True)\n","    wget.download('https://www.dropbox.com/s/mdsa4t1xmcimia6/audioset_16_16_0.4422.pth?dl=1', os.path.join('pretrained_weights', 'audioset_16_16_0.4422.pth'))\n","    print(\"done\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-02-08T11:23:05.789788Z","iopub.status.busy":"2025-02-08T11:23:05.789427Z","iopub.status.idle":"2025-02-08T11:23:08.707707Z","shell.execute_reply":"2025-02-08T11:23:08.706580Z","shell.execute_reply.started":"2025-02-08T11:23:05.789745Z"},"id":"X3fAn8D2Tb0I","trusted":true},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import torchaudio\n","from pathlib import Path\n","import torchvision.io as io\n","\n","\n","class UCF101Dataset(Dataset):\n","    def __init__(self, data_path, split_path, split=\"train\", num_frames=8, t=4):\n","        self.data_path = Path(data_path)\n","        self.num_frames = num_frames\n","        self.t = t\n","        # read split file\n","        split_file = \"trainlist01.txt\" if split == \"train\" else \"testlist01.txt\"\n","        with open(os.path.join(split_path, split_file), \"r\") as f:\n","            self.video_list = [line.strip().split(\" \")[0] for line in f.readlines()]\n","\n","        # TODO add more randomizations\n","        self.video_transform = transforms.Compose(\n","            [\n","                transforms.Resize((256, 256)), # slightly larger for random crop\n","                transforms.RandomCrop(224),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n","                ),\n","            ]\n","        )\n","\n","        # create class to index mapping\n","        self.class_to_idx = {}\n","        classes = sorted(\n","            list(set(video_name.split(\"/\")[0] for video_name in self.video_list))\n","        )\n","        for idx, classname in enumerate(classes):\n","            self.class_to_idx[classname] = idx\n","\n","    def _load_video(self, video_path):\n","        try:\n","            vframes, _, _ = io.read_video(str(video_path), pts_unit=\"sec\")\n","            total_frames = len(vframes)\n","\n","            # ensure we don't sample beyond video length\n","            if total_frames < self.num_frames:\n","                indices = torch.linspace(0, total_frames - 1, total_frames).long()\n","                indices = torch.cat(\n","                    [\n","                        indices,\n","                        torch.tensor(\n","                            [total_frames - 1] * (self.num_frames - total_frames)\n","                        ),\n","                    ]\n","                )\n","            else:\n","                indices = torch.linspace(0, total_frames - 1, self.num_frames).long()\n","\n","            frames = []\n","            for idx in indices:\n","                frame = vframes[idx]\n","                frame = Image.fromarray(frame.numpy())\n","                frame = self.video_transform(frame)\n","                frames.append(frame)\n","\n","        except Exception as e:\n","            print(e)\n","            frames = [torch.zeros(3, 224, 224) for _ in range(self.num_frames)]\n","\n","        return torch.stack(frames)  # [num_frames, c, h, w]\n","\n","    # TODO specaugment\n","    def _load_audio(self, video_path):\n","        try:\n","            audio_array, sample_rate = torchaudio.load(str(video_path))\n","        except (RuntimeError, TypeError):\n","            # create a small amount of noise instead of pure zeros\n","            audio_array = torch.randn(1, 16000 * self.t) * 1e-4\n","            sample_rate = 16000\n","\n","        # convert to mono\n","        if audio_array.shape[0] > 1:\n","            audio_array = torch.mean(audio_array, dim=0, keepdim=True)\n","\n","        # resample if necessary\n","        if sample_rate != 16000:\n","            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n","            audio_array = resampler(audio_array)\n","\n","        target_length = self.t * 16000\n","        if audio_array.shape[1] < target_length:\n","            # pad with zeros if audio is too short\n","            audio_array = torch.nn.functional.pad(\n","                audio_array, (0, target_length - audio_array.shape[1])\n","            )\n","        else:\n","            # trim if audio is too long\n","            audio_array = audio_array[:, :target_length]\n","\n","        # create mel spectrogram\n","        spectrogram = torchaudio.transforms.MelSpectrogram(\n","            sample_rate=16000,\n","            n_mels=128,\n","            n_fft=1024,\n","            win_length=1024,\n","            hop_length=160,\n","        )(audio_array)\n","\n","        spectrogram = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n","        spectrogram = spectrogram.squeeze(0)  # remove channel dimension\n","\n","        if spectrogram.shape[1] > 400:\n","            spectrogram = spectrogram[:, :400]\n","        elif spectrogram.shape[1] < 400:\n","            spectrogram = torch.nn.functional.pad(\n","                spectrogram, (0, 400 - spectrogram.shape[1])\n","            )\n","\n","        # mean=0 std=0.5 according to ast\n","        spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6) * 0.5\n","\n","        return spectrogram.unsqueeze(0)  # add channel dimension back [1, 128, 100*t]\n","\n","    def __getitem__(self, idx):\n","        video_name = self.video_list[idx]\n","        video_path = self.data_path / \"UCF-101\" / video_name\n","\n","        label = video_name.split(\"/\")[0]\n","        video_tensor = self._load_video(video_path)\n","        audio_tensor = self._load_audio(video_path)\n","        class_idx = self.class_to_idx[label]\n","\n","        return video_tensor, audio_tensor, class_idx\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","\n","def get_dataloaders(data_path, split_path, batch_size=2, num_workers=0):\n","\n","    train_dataset = UCF101Dataset(\n","        data_path=data_path,\n","        split_path=os.path.join(split_path, \"ucfTrainTestlist\"),\n","        split=\"train\",\n","    )\n","\n","    val_dataset = UCF101Dataset(\n","        data_path=data_path,\n","        split_path=os.path.join(split_path, \"ucfTrainTestlist\"),\n","        split=\"test\",\n","    )\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","    )\n","\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","    )\n","\n","    return train_loader, val_loader\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-02-08T11:23:08.709316Z","iopub.status.busy":"2025-02-08T11:23:08.708775Z","iopub.status.idle":"2025-02-08T11:23:09.921577Z","shell.execute_reply":"2025-02-08T11:23:09.920866Z","shell.execute_reply.started":"2025-02-08T11:23:08.709270Z"},"id":"udRiJHAlTdLe","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import timm\n","from timm.layers import trunc_normal_\n","\n","\n","# TODO: change hardcoded values\n","class PatchEmbed(nn.Module):\n","    def __init__(\n","        self, img_size=(128, 400), patch_size=(16, 16), in_chans=1, embed_dim=768\n","    ):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n","        self.num_patches = self.grid_size[0] * self.grid_size[1]\n","\n","        self.proj = nn.Conv2d(\n","            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n","        )\n","\n","    def forward(self, x):\n","        # B, C, H, W = x.shape\n","        # assert (\n","        #     H == self.img_size[0] and W == self.img_size[1]\n","        # ), f\"input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})\"\n","        x = self.proj(x)\n","        x = x.flatten(2).transpose(1, 2)\n","        return x\n","\n","\n","class Model(nn.Module):\n","    def __init__(self, num_classes=101, lf=10, num_bottlenecks=8):\n","        super().__init__()\n","\n","        # set num_classes=0 to remove classification head\n","        self.vv = timm.create_model(\n","            \"vit_base_patch16_224.augreg_in21k\", pretrained=True, num_classes=0\n","        )\n","\n","        self.va = timm.create_model(\n","            \"vit_base_patch16_224.augreg_in21k\", pretrained=True, num_classes=0\n","        )\n","\n","        # apply ast weights to va\n","        ast_weights = torch.load(\n","            \"pretrained_weights/audioset_16_16_0.4422.pth\", weights_only=True\n","        )\n","        temp = self.va.state_dict()\n","        pretrained_dict = {}\n","        for k, v in ast_weights.items():\n","            if k.startswith(\"module.\"):\n","                k = k[7:]\n","            if k in temp and temp[k].shape == v.shape:\n","                pretrained_dict[k] = v\n","        temp.update(pretrained_dict)\n","        self.va.load_state_dict(temp)\n","\n","        self.va.patch_embed = PatchEmbed(\n","            img_size=(128, 400),\n","            patch_size=(16, 16),\n","            in_chans=1,\n","            embed_dim=self.va.embed_dim,\n","        )\n","\n","        # interpolate position embeddings for video\n","        num_patches_video = self.vv.patch_embed.num_patches\n","        self.vv.pos_embed = self.interpolate_pos_encoding(\n","            self.vv.pos_embed, num_patches_video\n","        )\n","\n","        # interpolate position embeddings for audio\n","        num_patches_audio = self.va.patch_embed.num_patches\n","        self.va.pos_embed = self.interpolate_pos_encoding(\n","            self.va.pos_embed, num_patches_audio\n","        )\n","\n","        self.lf = lf\n","        self.num_features = self.vv.embed_dim\n","\n","        # create new classification head for fused features\n","        self.classifier = nn.Sequential(\n","            nn.LayerNorm(self.num_features),\n","            nn.Linear(self.num_features, num_classes),\n","        )\n","\n","        # create bottleneck fusion tokens\n","        self.num_bottlenecks = num_bottlenecks\n","        self.zfsn = nn.Parameter(torch.zeros(1, num_bottlenecks, self.vv.embed_dim))\n","        trunc_normal_(self.zfsn, std=0.02)\n","\n","    def interpolate_pos_encoding(self, pos_embed, num_patches):\n","        pos_embed = pos_embed.float()\n","        N = pos_embed.shape[1] - 1  # original number of patches (excluding CLS token)\n","\n","        # handle CLS token separately\n","        cls_pos_embed = pos_embed[:, 0:1, :]\n","        pos_embed = pos_embed[:, 1:, :]\n","\n","        # interpolate patch position embeddings\n","        pos_embed = pos_embed.permute(0, 2, 1)\n","        pos_embed = nn.functional.interpolate(\n","            pos_embed, size=num_patches, mode=\"linear\", align_corners=False\n","        )\n","        pos_embed = pos_embed.permute(0, 2, 1)\n","\n","        # recombine with CLS token\n","        pos_embed = torch.cat((cls_pos_embed, pos_embed), dim=1)\n","        return nn.Parameter(pos_embed)\n","\n","    def forward_features(self, x, v, lf):\n","        B = x.shape[0]\n","        x = v.patch_embed(x)\n","        if len(x.shape) > 3:  # just in case\n","            x = x.flatten(2).transpose(1, 2)\n","        cls_token = v.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_token, x), dim=1)\n","        x = v.pos_drop(x + v.pos_embed)\n","\n","        for i, block in enumerate(v.blocks):\n","            if i < lf:\n","                x = block(x)\n","        return x\n","\n","    def forward(self, video, audio):\n","        B, F, C, H, W = video.shape\n","        video = video.view(B * F, C, H, W)\n","\n","        # process separately until fusion layer\n","        v_features = self.forward_features(video, self.vv, self.lf)\n","        v_features = v_features.view(B, F, -1, self.num_features)\n","        v_features = torch.mean(v_features, dim=1)\n","        a_features = self.forward_features(audio, self.va, self.lf)\n","\n","        # expand fusion tokens for batch\n","        zfsn = self.zfsn.expand(B, -1, -1)\n","\n","        # process remaining layers with bottleneck fusion\n","        for block in self.vv.blocks[self.lf :]:\n","            # eqn 8\n","            v_concat = torch.cat([v_features, zfsn], dim=1)\n","            a_concat = torch.cat([a_features, zfsn], dim=1)\n","\n","            v_out = block(v_concat)\n","            a_out = block(a_concat)\n","\n","            # split features and fusion tokens\n","            v_features = v_out[:, : v_features.shape[1]]\n","            a_features = a_out[:, : a_features.shape[1]]\n","            v_zfsn = v_out[:, v_features.shape[1] :]\n","            a_zfsn = a_out[:, a_features.shape[1] :]\n","\n","            # eqn 9\n","            zfsn = (v_zfsn + a_zfsn) / 2\n","\n","        # get CLS tokens\n","        v_cls = v_features[:, 0]\n","        a_cls = a_features[:, 0]\n","\n","        v_logits = self.classifier(v_cls)\n","        a_logits = self.classifier(a_cls)\n","\n","        output = (v_logits + a_logits) / 2\n","\n","        return output\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-02-08T11:23:10.111872Z","iopub.status.busy":"2025-02-08T11:23:10.111575Z","iopub.status.idle":"2025-02-08T11:23:10.126730Z","shell.execute_reply":"2025-02-08T11:23:10.125705Z","shell.execute_reply.started":"2025-02-08T11:23:10.111850Z"},"id":"SP90zlKMTgK8","trusted":true},"outputs":[],"source":["import os\n","import datetime\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","import wandb\n","\n","\n","class Trainer:\n","    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n","        wandb.login(key=\"2f3ffd7baf545af396e18e48bfa20b33d2609dcc\")\n","\n","        self.model = model.to(device)\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.device = device\n","\n","        if wandb.run is None:\n","            current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","            run_name = f\"wabt_{current_time}\"\n","\n","            wandb.init(project=\"wabt\", name=run_name)\n","            wandb.watch(self.model)\n","        self.step = 0\n","\n","        self.best_val_loss = float(\"inf\")\n","        self.checkpoint_dir = os.path.join(\"checkpoints\", run_name)\n","        os.makedirs(self.checkpoint_dir, exist_ok=True)\n","\n","    def train_epoch(self):\n","        self.model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        pbar = tqdm(self.train_loader, desc=\"training\")\n","        for video, audio, targets in pbar:\n","            video = video.to(self.device)\n","            audio = audio.to(self.device)\n","            targets = targets.to(self.device)\n","\n","            self.optimizer.zero_grad()\n","            outputs = self.model(video, audio)\n","            loss = self.criterion(outputs, targets)\n","\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            self.step += 1\n","            wandb.log(\n","                {\n","                    \"batch/train_loss\": loss.item(),\n","                    \"batch/train_acc\": predicted.eq(targets).sum().item()\n","                    / targets.size(0),\n","                    \"train/running_loss\": running_loss / (pbar.n + 1),\n","                    \"train/running_acc\": correct / total,\n","                    \"step\": self.step,\n","                }\n","            )\n","\n","            pbar.set_postfix(\n","                {\"loss\": running_loss / (pbar.n + 1), \"acc\": correct / total}\n","            )\n","\n","        epoch_loss = running_loss / len(self.train_loader)\n","        epoch_acc = correct / total\n","        return epoch_loss, epoch_acc\n","\n","    def validate(self):\n","        self.model.eval()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            pbar = tqdm(self.val_loader, desc=\"validation\")\n","            for video, audio, targets in pbar:\n","                video = video.to(self.device)\n","                audio = audio.to(self.device)\n","                targets = targets.to(self.device)\n","\n","                outputs = self.model(video, audio)\n","                loss = self.criterion(outputs, targets)\n","\n","                running_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","\n","                self.step += 1\n","                wandb.log(\n","                    {\n","                        \"batch/val_loss\": loss.item(),\n","                        \"batch/val_acc\": predicted.eq(targets).sum().item()\n","                        / targets.size(0),\n","                        \"val/running_loss\": running_loss / (pbar.n + 1),\n","                        \"step\": self.step,\n","                    }\n","                )\n","\n","                pbar.set_postfix(\n","                    {\"loss\": running_loss / (pbar.n + 1), \"acc\": correct / total}\n","                )\n","\n","        epoch_loss = running_loss / len(self.val_loader)\n","        epoch_acc = correct / total\n","\n","        # save checkpoint\n","        checkpoint = {\n","            \"model_state_dict\": self.model.state_dict(),\n","            \"optimizer_state_dict\": self.optimizer.state_dict(),\n","            \"val_loss\": epoch_loss,\n","            \"val_acc\": epoch_acc,\n","            \"step\": self.step,\n","        }\n","\n","        # save latest checkpoint\n","        latest_path = os.path.join(self.checkpoint_dir, \"latest.pt\")\n","        torch.save(checkpoint, latest_path)\n","\n","        # save best checkpoint\n","        if epoch_loss < self.best_val_loss:\n","            self.best_val_loss = epoch_loss\n","            best_path = os.path.join(self.checkpoint_dir, \"best.pt\")\n","            torch.save(checkpoint, best_path)\n","            wandb.log({\"best_val_loss\": self.best_val_loss})\n","\n","        return epoch_loss, epoch_acc\n","\n","\n","def train(model, train_loader, val_loader, num_epochs=10, device=\"cuda\"):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n","    trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)\n","\n","    for epoch in range(num_epochs):\n","        print(f\"\\nepoch {epoch+1}/{num_epochs}\")\n","        train_loss, train_acc = trainer.train_epoch()\n","        val_loss, val_acc = trainer.validate()\n","\n","        wandb.log(\n","            {\n","                \"epoch\": epoch + 1,\n","                \"epoch/train_loss\": train_loss,\n","                \"epoch/train_acc\": train_acc,\n","                \"epoch/val_loss\": val_loss,\n","                \"epoch/val_acc\": val_acc,\n","            }\n","        )\n","\n","        print(f\"train loss: {train_loss:.4f} | train acc: {train_acc:.2f}\")\n","        print(f\"val loss: {val_loss:.4f} | val acc: {val_acc:.2f}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"execution":{"iopub.execute_input":"2025-02-08T11:23:17.742805Z","iopub.status.busy":"2025-02-08T11:23:17.742433Z","iopub.status.idle":"2025-02-08T14:38:24.640160Z","shell.execute_reply":"2025-02-08T14:38:24.639344Z","shell.execute_reply.started":"2025-02-08T11:23:17.742776Z"},"id":"y1K4x0qKThWZ","outputId":"4d656f62-9348-4e93-82f6-1ae95b0d8776","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msouhhmm\u001b[0m (\u001b[33msouhhmm-bits-pilani\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20250208_112328-ivn84c4g</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/souhhmm-bits-pilani/wabt/runs/ivn84c4g' target=\"_blank\">wabt_20250208_112328</a></strong> to <a href='https://wandb.ai/souhhmm-bits-pilani/wabt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/souhhmm-bits-pilani/wabt' target=\"_blank\">https://wandb.ai/souhhmm-bits-pilani/wabt</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/souhhmm-bits-pilani/wabt/runs/ivn84c4g' target=\"_blank\">https://wandb.ai/souhhmm-bits-pilani/wabt/runs/ivn84c4g</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","epoch 1/3\n"]},{"name":"stderr","output_type":"stream","text":["training: 100%|██████████| 4769/4769 [51:28<00:00,  1.54it/s, loss=1.92, acc=0.536] \n","validation: 100%|██████████| 1892/1892 [13:14<00:00,  2.38it/s, loss=1.02, acc=0.725] \n"]},{"name":"stdout","output_type":"stream","text":["train loss: 1.9194 | train acc: 0.54\n","val loss: 1.0198 | val acc: 0.72\n","\n","epoch 2/3\n"]},{"name":"stderr","output_type":"stream","text":["training: 100%|██████████| 4769/4769 [51:38<00:00,  1.54it/s, loss=0.281, acc=0.923] \n","validation: 100%|██████████| 1892/1892 [13:13<00:00,  2.39it/s, loss=0.924, acc=0.742]\n"]},{"name":"stdout","output_type":"stream","text":["train loss: 0.2811 | train acc: 0.92\n","val loss: 0.9236 | val acc: 0.74\n","\n","epoch 3/3\n"]},{"name":"stderr","output_type":"stream","text":["training: 100%|██████████| 4769/4769 [51:41<00:00,  1.54it/s, loss=0.1, acc=0.973]    \n","validation: 100%|██████████| 1892/1892 [13:13<00:00,  2.38it/s, loss=0.622, acc=0.831]\n"]},{"name":"stdout","output_type":"stream","text":["train loss: 0.1003 | train acc: 0.97\n","val loss: 0.6221 | val acc: 0.83\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = Model().to(device)\n","train_loader, val_loader = get_dataloaders(data_path=\"data/\", split_path=\"data/\")\n","\n","train(model, train_loader, val_loader, num_epochs=3, device=device)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
